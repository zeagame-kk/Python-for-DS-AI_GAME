{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Let's modify the above scratch code to\n",
    "\n",
    "- Modify the scratch code so it can accept an hyperparameter max_depth, in which it will continue create the tree until max_depth is reached.\n",
    "\n",
    "- Put everything into a class DecisionTree. It should have at least two methods, fit(), and predict()\n",
    "\n",
    "- Load the iris data and try with your class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "        \n",
    "    def find_split(self, X, y):\n",
    "        n_samples = y.size\n",
    "        if n_samples <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        feature_ix, threshold = None, None\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(self.n_classes)]\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\n",
    "        for feature in range(self.n_features):\n",
    "        \n",
    "            # Sort data along selected feature.\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\n",
    "            sort_idx = np.argsort(X[:, feature])\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\n",
    "                \n",
    "            sample_per_class_left = [0] * self.n_classes   #[0, 0]\n",
    "        \n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\n",
    "            \n",
    "            for i in range(1, n_samples):\n",
    "                #the class of that sample\n",
    "                c = y_sorted[i - 1]  #[0]\n",
    "            \n",
    "                #put the sample to the left\n",
    "                sample_per_class_left[c] += 1  #[1, 0]\n",
    "                        \n",
    "                #take the sample out from the right  [1, 2]\n",
    "                sample_per_class_right[c] -= 1\n",
    "            \n",
    "                gini_left = 1.0 - sum(\n",
    "                    (sample_per_class_left[x] / i) ** 2 for x in range(self.n_classes)\n",
    "                )\n",
    "                        \n",
    "                #we divided by n_samples - i since we know that the left amount of samples\n",
    "                #since left side has already i samples\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(self.n_classes)\n",
    "                )\n",
    "\n",
    "                #weighted gini\n",
    "                weighted_gini = ((i / n_samples) * gini_left) + ( (n_samples - i) /n_samples) * gini_right\n",
    "            \n",
    "\n",
    "                # in case the value are the same, we do not split\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    feature_ix = feature\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2  # midpoint\n",
    "\n",
    "        #return the feature number and threshold \n",
    "        #used to find best split\n",
    "        return feature_ix, threshold\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes)]\n",
    "        #predicted class using the majority of sample class\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        #define the parent node\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        \n",
    "        #From solution\n",
    "        if depth < self.max_depth:\n",
    "            \n",
    "            #perform recursion\n",
    "            feature, threshold = self.find_split(X, y)\n",
    "            if feature is not None:\n",
    "                #take all the indices that is less than threshold\n",
    "                indices_left = X[:, feature] < threshold\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "\n",
    "                #tilde for negation\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "\n",
    "                #take note for later decision\n",
    "                node.feature_index = feature\n",
    "                node.threshold = threshold\n",
    "                node.left = self.grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self.grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "        \n",
    "    #to predict, it is as simple as moving \n",
    "    #through the tree \n",
    "    def _predict(self, sample):\n",
    "        tree = self.tree\n",
    "        while tree.left:\n",
    "            if sample[tree.feature_index] < tree.threshold:\n",
    "                tree = tree.left\n",
    "            else:\n",
    "                tree = tree.right\n",
    "        return tree.predicted_class\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict = [self._predict(sample) for sample in X]\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    dataset = load_iris()\n",
    "    X, y = dataset.data, dataset.target\n",
    "    clf = DecisionTree(max_depth=10)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.predict([[0, 0, 5, 1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature used for split:  2\n",
      "Best threshold used for split:  2.45\n"
     ]
    }
   ],
   "source": [
    "feature, threshold = clf.find_split(X, y)\n",
    "\n",
    "print(\"Best feature used for split: \", feature)\n",
    "print(\"Best threshold used for split: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
