{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Modify the AdaBoost scratch code in our lecture such that:\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.  Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index.   To check whether your stump really works, it should give you still relatively the same accuracy.  In addition, if you do not change y to -1, it will result in very bad accuracy.  Unlike sklearn version of DecisionTree, it will STILL work even y is not change to -1 since it uses gini index\n",
    "- Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create claa Stump including all features, threshold, and polarity, identify the best stump which has lowest weighted errors\n",
    "class Stump:\n",
    "    def __init__(self):\n",
    "        #note from solution: Determines whether threshold should be evaluated as < or >\n",
    "        self.features = None\n",
    "        self.threshold = None\n",
    "        self.polarity = 1\n",
    "        \n",
    "        #note from solution: Voting power of the stump\n",
    "        self.alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, S=5, eta=0.5):\n",
    "        self.S = S\n",
    "        self.eta = eta\n",
    "        \n",
    "    def fit(self, X, y):   #<--- for training X_train and y_train\n",
    "        m, n = X.shape\n",
    "        \n",
    "        #initially, we set our weight to 1/m\n",
    "        W = np.full(m, 1/m)\n",
    "        \n",
    "        #holder for all clfs we have tried\n",
    "        self.clfs = []\n",
    "        \n",
    "        for _ in range(self.S):\n",
    "            clf = Stump()\n",
    "            \n",
    "            #from solution\n",
    "            #set initially minimum error to infinity\n",
    "            #so at least the first stump is identified\n",
    "            min_err = np.inf\n",
    "\n",
    "            #previously we don't need to do this\n",
    "            #since sklearn learn does it\n",
    "            #but now we have to loop all features, all threshold\n",
    "            #and all polarity to find the minimum weighted errors\n",
    "            for feature in range(n):\n",
    "                feature_vals = np.sort(np.unique(X[:, feature]))\n",
    "                thresholds = (feature_vals[:-1] + feature_vals[1:])/2\n",
    "                for threshold in thresholds:\n",
    "                    for polarity in [1, -1]:\n",
    "                        yhat = np.ones(len(y)) #set all to 1\n",
    "                        yhat[polarity * X[:, feature] < polarity * threshold] = -1  #polarity=1 rule\n",
    "                        err = W[(yhat != y)].sum()\n",
    "                                        \n",
    "                        #save the best stump\n",
    "                        if err < min_err:\n",
    "                            clf.polarity = polarity\n",
    "                            clf.threshold = threshold\n",
    "                            clf.feature_index = feature\n",
    "                            min_err = err\n",
    "        #once we know which is the best stump\n",
    "        #we calculate its alpha, and reweight samples\n",
    "        eps = 1e-10 #to prevent division by zero\n",
    "        clf.alpha = self.eta * (np.log ((1 - err) / (err + eps)))\n",
    "        W = W * np.exp(-clf.alpha * y * yhat) \n",
    "        W = W / sum (W)\n",
    "        \n",
    "        #save clf\n",
    "        self.clfs.append(clf)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        m, n = X.shape\n",
    "        yhat = np.zeros(m)\n",
    "        \n",
    "        for clf in self.clfs:\n",
    "            pred = np.ones(m) #set all to 1\n",
    "            pred[clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold] = -1  #polarity=1 rule\n",
    "            yhat += clf.alpha * pred\n",
    "            \n",
    "        return np.sign(yhat)     #<---The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.95      0.94        79\n",
      "           1       0.94      0.93      0.94        71\n",
      "\n",
      "    accuracy                           0.94       150\n",
      "   macro avg       0.94      0.94      0.94       150\n",
      "weighted avg       0.94      0.94      0.94       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = AdaBoost(S=10)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada score:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#SAMME.R - a variant of SAMME which relies on class probabilities \n",
    "#rather than predictions and generally performs better\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
