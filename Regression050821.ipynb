{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training. Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "#1.1 Get X and y in the right shape\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "m = X.shape[0]    #number of samples\n",
    "n = X.shape[1]    #number of features\n",
    "y = boston.target\n",
    "assert m == y.shape[0]     #number of rows in X is the same as number of rows in y\n",
    "\n",
    "\n",
    "#1.2 Feature scale my data to reach faster convergence\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)     #Fit to data then transform it\n",
    "\n",
    "\n",
    "#1.3 Train test split my data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "\n",
    "#1.4 Add intercepts\n",
    "#w0 is my intercept\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "#np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "#concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    #select method: 'batch', 'sgd', 'mini_batch'\n",
    "    def __init__(self, alpha = 0.001, max_iter = 1000, tol = 0.0001, iter_stop = 10**5, method = 'batch', n_sample = 100):\n",
    "   \n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.iter_stop = iter_stop\n",
    "        self.method = method\n",
    "        self.n_sample = n_sample\n",
    "        \n",
    "    def h_theta(self, X, theta):\n",
    "        return X @ theta\n",
    "\n",
    "    def mse(self, yhat, y):\n",
    "        return ((yhat - y)**2).sum() / np.shape(yhat)[0]\n",
    "\n",
    "    def mse_sgd(self, yhat, y):\n",
    "        return (yhat - y)**2\n",
    "\n",
    "    def gradient(self, X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "    def gradient_sgd(self, X, error):\n",
    "        return np.array(np.matrix(list(X)).T @ [error]).reshape(-1)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        yhat = self.h_theta(X_test, self.theta)\n",
    "        return yhat\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        start = time()\n",
    "        assert X_train.shape[0] == y_train.shape[0]\n",
    "               \n",
    "        self.theta = np.zeros(X_train.shape[1])\n",
    "        self.mse_hist_train = []      #without replacement\n",
    "        random_list_train = list(np.arange(0, len(X_train)))    \n",
    "         \n",
    "        if self.method == 'batch':\n",
    "            for i in range(self.max_iter):\n",
    "                yhat = self.h_theta(X_train, self.theta)\n",
    "                error = yhat - y_train\n",
    "                grad = self.gradient(X_train, error)\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                # MSE of Training\n",
    "                mse_train = self.mse(yhat, y_train)\n",
    "                self.mse_hist_train.append(mse_train)\n",
    "\n",
    "                # Check condition\n",
    "                # Condition 1: if loss_diff < tolerance (consider -> training set)\n",
    "                if i == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    loss_diff = np.abs(self.mse_hist_train[-1] - self.mse_hist_train[-2])\n",
    "                    if (loss_diff <= self.tol):\n",
    "                        break\n",
    "\n",
    "                # Condition 2: stop in the iteration that we want\n",
    "                if i == self.iter_stop:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "        elif self.method == 'sdg':\n",
    "            for i in range(self.max_iter):\n",
    "                for j in random_list_train:\n",
    "                    # Random data : Stochastic Gradient Descent\n",
    "                    random_index_train = random.choice(random_list_train)\n",
    "                    random_list_train.remove(random_index_train)\n",
    "\n",
    "                    # Reset random_list again if no element in the list anymore\n",
    "                    if len(random_list_train) == 0:\n",
    "                        random_list_train = list(np.arange(0, len(X_train)))\n",
    "\n",
    "                    yhat = self.h_theta(X_train[random_index_train], self.theta)\n",
    "                    error = yhat - y_train[random_index_train]\n",
    "                    grad = self.gradient_sgd(X_train[random_index_train], error)\n",
    "                    self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                # MSE of Training\n",
    "                mse_train = self.mse_sgd(yhat, y_train[random_index_train])\n",
    "                self.mse_hist_train.append(mse_train)\n",
    "\n",
    "                # Check condition\n",
    "                # Condition 1: if loss_diff < tolerance (condider -> training set)\n",
    "                if i == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    loss_diff = np.abs(self.mse_hist_train[-1] - self.mse_hist_train[-2])\n",
    "                    if (loss_diff <= self.tol):\n",
    "                        break\n",
    "\n",
    "                # Condition 2: stop in the iteration that we want\n",
    "                if i == self.iter_stop-1:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "        elif self.method == 'mini_batch':\n",
    "            for i in range(self.max_iter):\n",
    "                # Random data : Mini-Batch Stochastic Gradient Descent\n",
    "                if self.n_sample > len(X_train):\n",
    "                    self.n_sample = len(X_train)\n",
    "\n",
    "                random_index_batch = random.sample(random_list_train, k = self.n_sample)\n",
    "                yhat = self.h_theta(X_train[random_index_batch], self.theta)\n",
    "                error = yhat - y_train[random_index_batch]\n",
    "                grad = self.gradient(X_train[random_index_batch], error)\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                # MSE of Training\n",
    "                mse_train = self.mse(yhat, y_train[random_index_batch])\n",
    "                self.mse_hist_train.append(mse_train)\n",
    "\n",
    "                # Check condition\n",
    "                # Condition 1: if loss_diff < tolerance (condider -> training set)\n",
    "                if i == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    loss_diff = np.abs(self.mse_hist_train[-1] - self.mse_hist_train[-2])\n",
    "                    if (loss_diff <= self.tol):\n",
    "                        break\n",
    "\n",
    "                # Condition 2: stop in the iteration that we want\n",
    "                if i == self.iter_stop:\n",
    "                    break   \n",
    "                    \n",
    "                    \n",
    "        time_taken = time() - start  \n",
    "        print(\"Stop at iteration: \", i+1)\n",
    "        print(\"Time used: \", time_taken)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at iteration:  804\n",
      "Time used:  0.03491067886352539\n",
      "MSE:  22.2190616714931\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# Early stopping\n",
    "batch_model = LinearRegression(alpha = 0.0001, max_iter = 1000,  tol = 0.0001, method = 'batch')\n",
    "batch_model.fit(X_train, y_train)\n",
    "y_hat = batch_model.predict(X_test)\n",
    "mse = batch_model.mse(y_hat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at iteration:  1000\n",
      "Time used:  2.3546857833862305\n",
      "MSE:  22.5103281567019\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# SDG\n",
    "sdg_model = LinearRegression(alpha = 0.0001, max_iter = 1000,  tol = 0.0001, method = 'sdg')\n",
    "sdg_model.fit(X_train, y_train)\n",
    "y_hat = sdg_model.predict(X_test)\n",
    "mse = sdg_model.mse(y_hat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at iteration:  1000\n",
      "Time used:  0.22979116439819336\n",
      "MSE:  22.109245739557924\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# Mini-Batch\n",
    "mini_model = LinearRegression(alpha = 0.0001, max_iter = 1000,  tol = 0.0001, method = 'mini_batch', n_sample = 100)\n",
    "mini_model.fit(X_train, y_train)\n",
    "y_hat = mini_model.predict(X_test)\n",
    "mse = mini_model.mse(y_hat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
